# -*- coding: utf-8 -*-
"""smilerec_with_tf_dropout,activation,l1l2para.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gVnVrMahbldfJjhcCsF-cb9foT85AebN
"""

import tensorflow as tf

sess = tf.Session()

import PIL

n_inputs=64*64*3
n_hidden1=500
n_hidden2=25
n_outputs=2

X=tf.placeholder(tf.float32,shape=(None,n_inputs),name="X")
y=tf.placeholder(tf.int64,shape=(None),name="y")
training = tf.placeholder_with_default(False,shape=(),name='training')

def leaky_relu(z,name=None):
  return tf.maximum(0.01*z,z,name=name)

dropout_rate = 0.5
X_drop = tf.layers.dropout(X,dropout_rate,training= training)

from functools import partial
scale = 0.001
my_dense_layer = partial(
    tf.layers.dense, activation=leaky_relu,
    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))

from functools import partial
with tf.name_scope("dnn"):
  he_init = tf.variance_scaling_initializer()
  my = partial(tf.layers.batch_normalization,training=training,momentum=0.9)
  hidden1=my_dense_layer(X_drop,n_hidden1,name="hidden1")
  bn1 = my(hidden1)
  bn1_act = tf.nn.elu(bn1)
  bn_1_drop = tf.layers.dropout(bn1_act,dropout_rate,training = training)
  hidden2=my_dense_layer(bn_1_drop,n_hidden2,name="hidden2")
  bn2 = my(hidden2)
  bn2_act = tf.nn.elu(bn2)
  bn_2_drop = tf.layers.dropout(bn2_act,dropout_rate,training = training)
  logits_before_bn= my_dense_layer(bn_2_drop,n_outputs,name="outputs")
  logits = tf.layers.batch_normalization(logits_before_bn,training=training,momentum=0.9)
  y_proba = tf.nn.softmax(logits)

learning_rate =0.01

with tf.name_scope("loss"):                                     # not shown in the book
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown
        labels=y, logits=logits)                                # not shown
    base_loss = tf.reduce_mean(xentropy, name="avg_xentropy")   # not shown
    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
    loss = tf.add_n([base_loss] + reg_losses, name="loss")

with tf.name_scope("train"):
  threshold=1.0
  optimizer = tf.train.GradientDescentOptimizer(learning_rate)
  grads_and_vars = optimizer.compute_gradients(loss)
  capped_gvs = [(tf.clip_by_value(grad,-threshold,threshold),var) 
                for grad,var in grads_and_vars]
  training_op=optimizer.apply_gradients(capped_gvs)

with tf.name_scope("eval"):
  correct = tf.nn.in_top_k(logits,y,1)
  accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))

init = tf.global_variables_initializer()
saver = tf.train.Saver()

n_epochs = 40
batch_size = 50

from PIL import Image

from google.colab import drive
drive.mount('/content/gdrive')

import numpy

def img_loader():
    common_path = "/content/gdrive/My Drive/Smile_Recognition/faces/"
    txt_path = "/content/gdrive/My Drive/Smile_Recognition/"
    id_add = open(txt_path+"ALL_LIST_ppm.txt",'r')
    print("Txtfile Opened!!!")
    images = []
    for line in id_add:
      print(line.strip())
      if True:
        grand_row = []
        img = Image.open(common_path+line.strip())
        img= (numpy.array(img))
        #print img[0][0]
        for row in img:
            for pixel in row:
                for color in pixel:
            #print row
                    grand_row.append(color)
        images.append(grand_row)
        
    id_add.close()
    #print(len(grand_row))
    return images

def smile_check():
    txt_path = "/content/gdrive/My Drive/Smile_Recognition/"
    smile = open(txt_path+"SMILE_list_ppm.txt",'r')
    print("Smile List Opened")
    id_add = open(txt_path+"ALL_LIST_ppm.txt",'r')
    print("All list opened")
    smile_list = []
    for line in smile:
        smile_list.append(line.strip())
    smiles = []
    x=0
    for name in id_add:
        if name.strip() in smile_list:
            smiles.append(True)
            #print (x)
        else:
            smiles.append(False)
        x+=1
    #print(len(smile_list))
    smile.close()
    id_add.close()
    return smiles

def shuffle_batch(X, y, batch_size):
    rnd_idx = np.random.permutation(len(X))
    n_batches = len(X) // batch_size
    print(type(X))
    print(type(y))
    for batch_idx in np.array_split(rnd_idx, n_batches):
        X_batch, y_batch = X[batch_idx], y[batch_idx]
        yield X_batch, y_batch

import numpy as np
def main():
    images = img_loader()
    smiles = smile_check()
    temp_X = []
    temp_y =[]
    chaos = np.random.permutation(len(images))
    print("Images Loaded!")
    for i in chaos:
      temp_X.append(images[i])
      temp_y.append(smiles[i])
    images=temp_X
    smiles=temp_y
    train_X = images[:900]
    test_X = images[900:]
    train_Y = smiles[:900]
    test_Y = smiles[900:]
    print("Images Shuffled")
    print(train_X[0])
    print(train_Y[:101])
    n_epochs = 10
    batch_size = 800
    with tf.Session() as sess:
      init.run()
      for epoch in range(n_epochs):
        n=0
        for iteration in range(len(train_X)//batch_size):
          
          X_batch=train_X[:n+batch_size+1]
          Y_batch = train_Y[:n+batch_size+1]
          sess.run(training_op,feed_dict={X:X_batch,y:Y_batch})
          n=n+batch_size+1
        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: Y_batch})
        acc_valid = accuracy.eval(feed_dict={X: test_X, y: test_Y})
        print(epoch, "Batch accuracy:", acc_batch, "Validation accuracy:", acc_valid)
      save_path = saver.save(sess,"./my_moel_final.ckpt")

main()

def xtest():
  images = img_loader()
  smiles = smile_check()
  train_X = images[:900]
  print (train_X)
  print(train_X[0])